---
title: "Compositional analysis of microbiome data"
author: "Xin (David) Zhao"
date: "Last edited `r format(Sys.time(), '%d %B %Y')`"
knit: (function(inputFile, encoding) {
      out_dir <- 'docs';
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_file=file.path(dirname(inputFile), out_dir, 'index.html'))})
output:
  html_document:
    # theme: cosmo
    highlight: pygments
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    collapsed: FALSE
    number_sections: FALSE
    fig_width: 7
    fig_height: 6
    fig_caption: TRUE
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

<html>

<head>

```{=html}
<style>

h1{
 color: #055C9D;
 font-family: Georgia;
 font-size: 200%
}


h2{
 color: #055C9D;
 font-family: helvetica;
 font-size: 150%
}

h3{
 color: #055C9D;  
 font-family: helvetica;
 font-size: 120%; 
}

p {
 color: #333333;
 font-family: helvetica;
 font-size: 100%;
}

</style>
```
</head>

</html>

```{r setup, include = FALSE}
# set options for the entire document 
knitr::opts_chunk$set(fig.align = 'center', 
                      fig.dim =c(6,4), 
                      dev="png",
                      echo=TRUE, #display code in output document 
                      error=FALSE,
                      message=FALSE) #stop render when error occurs   
```

```{r libraries, message=FALSE, warning=FALSE}

# This project was built with R 4.2.1 

library(ALDEx2)
library(vegan)
library(zCompositions) # for zero treatment in microbiome data 
library(compositions) 
library(DiagrammeR) # for flow diagram 
library(dendextend) # set parameters for plotting dendrograms 
library(scales)
library(RColorBrewer)
library(tidyverse)

```

## Introduction

**Compositional data** exists as the proportions, or fractions, of a
whole or portions of a total, conveying exclusively relative
information, and have the properties: the elements of composition are
non-negative and sum to unity.[@xia2018] Mathematically, compositional
data is represented by points on a simplex (i.e., **Aitchison
simplex**). Measurements involving probabilities, proportions,
percentages, and ppm (10^6^, parts per million), ppb (10^9^, parts per
billion)([Wikipedia](https://en.wikipedia.org/wiki/Compositional_data)).

Composition data violate the assumptions of all standard statistical
tests because [@xia2018]:

-   spurious correlations preclude correlation analysis
-   graphical distortions make the visualizing tools impossible (e.g.,
    scatter plot; QQ plot et al.)
-   lack of multivariate normality of compositions prevents multivariate
    parametric modeling of compositional data
-   dependence of the mixture makes ANOVA and linear regression
    meaningless

One critical progress in compositional data analysis since the 1980s to
use the **log-ratio transformations**.The book, "Statistical Analysis of
Microbiome Data with R" [@xia2018] provides a list for a family of
**log-ratio transformations** popular in microbiome studies as follows, 

-   Additive Log-ratio (alr) transformation
-   Centered Log-ratio (clr) transformation
-   Isometric Log-ratio (ilr) transformation

The difference among these three log-ratio transformations is to choose
the **divisor**. In other words, it to choose which value to be used to
normalize all the values in a sample. Each transformation has its own
weakness or advantages [@xia2018].

Note that log and log-ratio transformations require non-zero elements in
the data matrix. As a result, compositional data analysis must be
preceded by **a treatment of the zeros**. Three sources of zeros in
microbiome data includes: 

-   Rounded zeros
-   Sampling zeros
-   Structural zeros

## R workflow

```{r flow diagram, echo=FALSE}

# flow diagram instruction https://epirhandbook.com/en/diagrams-and-charts.html 

DiagrammeR::grViz("digraph{

graph[layout = dot, 
      rankdir = TB]

node[shape = box,
     fixedsize = true,
     width = 20,
     height = 7,
     color = RoyalBlue]
    
a [label = 'Load\nData',
  fontsize = 180]
b [label = 'Treat\nZeroness',
  fontsize = 180]
c [label = 'ConvertDatato\nProportions',
  fontsize = 180]
d [label = 'Filter\nData',
  fontsize = 180]
e [label = 'clr\nTransformation',
  fontsize = 180]
f [label = 'PCA',
  fontsize = 180]
g [label = 'Display\nPCA',
  fontsize = 180]
h [label = 'Welchs\nttest',
  fontsize = 180]
i [label = 'Estimate\nEffectSize',
  fontsize = 180]
j [label = 'Difference\nPlot',
  fontsize = 180]

a -> b -> c -> d -> e -> f -> g -> h -> i -> j
}")

```

## Scripts to implement above procedures

### Step 1. Download raw microbiome data and metadata

The microbiome OTU table and metadata was retrieved from [ML
Repo](https://knights-lab.github.io/MLRepo/docs/turnbaugh_lean_obese_all.html)

```{r}

# raw OTU table
raw_otu <- read.csv(file = "https://knights-lab.github.io/MLRepo/datasets/turnbaugh/refseq/otutable.txt",
                    header=T,
                    sep = "")

```

Read in the metadata from URL.

```{r}

# metadata 

meta <- read.csv(file = "https://knights-lab.github.io/MLRepo/datasets/turnbaugh/task-obese-lean-all.txt",
         header = TRUE,
         sep = "") 

```

The metadata contains `r dim(meta)[1]` samples. `Var` indicates the
independent, binary variable of interest, `Lean` and `Obese`.

### Step 2. Preprocess input data

Convert the raw OTU table, with samples being rows while OTU being
columns.

```{r}

t_raw_otu <- raw_otu %>% 
  gather(key = "sample",
         value = "reads",
         3:ncol(.)) %>% 
  dplyr::select(-ID) %>% 
  spread(key = names(.)[1],
         value = "reads") %>% 
  column_to_rownames("sample")  # convert column to row id 

```

Inspect whether any NA (missing values) exits; replace NA with 0 values
if any. Follow the instructions introduced
[here](https://www.r-bloggers.com/2019/09/handling-missing-values-in-r-using-tidyr/).

```{r, results='hide'}

sum(is.na(t_raw_otu)) # check number of missing values 

na_t_raw_otu <- t_raw_otu %>% 
  mutate_all(replace_na, 0) # replace NA with 0 value 

sum(is.na(na_t_raw_otu)) # check number of missing values afterwards 


```

Evaluate visually 0/missing values patterns in the OTU table with
`zPatterns()` from the `zCompositions` package.

```{r zPatterns, results='hide'}

pattern.ID <- zPatterns(na_t_raw_otu, 
          label = 0, # identify zero percentages 
          axis.labels = c("Taxa", "Sample ID"),
          plot=TRUE,
          show.means = FALSE,
          bar.ordered = c(TRUE, TRUE),
          bar.labels = FALSE,
          suppress.print = TRUE)  

```

Apply `cmultRepl()` from the `zCompositions` package to replace 0 values
with the count zero multiplicative method and output counts. 

Error information, however, appear and the function stops. Therefore,
remove samples that are zeros for every OTU. And then implement the
above function again.

```{r drop OTU with all zeros, collapse=TRUE}

rm_na_t_raw_otu <- na_t_raw_otu[pattern.ID != 281, ] 


rp_rm_na_t_raw_otu <- cmultRepl(rm_na_t_raw_otu, 
          method = "CZM", # count zero multiplicative method 
          output = "p-counts",  # pseudo-counts 
          z.warning=0.8)  

```

### Step 3. Convert data to proportions

Transpose the resulting data frame to OTU X sample format.

```{r}

pcount_otu <- rp_rm_na_t_raw_otu %>% 
  rownames_to_column("sample") %>% 
  gather(key = "otu", value = "reads", -sample) %>% 
  spread(key = "sample", value = "reads") %>% 
  column_to_rownames("otu") 

head(pcount_otu)
```

Calculate total pcount throughout samples.

```{r prop}

pcount_otu %>% 
  rownames_to_column("otu") %>% 
  gather(key= "sample", value = "pcount", -otu) %>% 
  group_by(sample) %>% 
  summarise(total = sum(pcount), .groups = "drop") 

```

Calculate proportions based on pseudo counts.

```{r, collapse=TRUE}

prop_pcount_otu <- pcount_otu %>% 
  map_dfr(function(x) x/sum(x)) 
  
rownames(prop_pcount_otu) <- rownames(pcount_otu)  # assign row names to the resulting data frame 

```

### Step 4. Filter rare taxa

Filter proportional OTU data to remove all OTU that are less than 0.01%
in any samples.

```{r filtering}

otu_id_dropped <- prop_pcount_otu %>% 
  rownames_to_column("otu") %>% 
  gather(key = "sample", value="prop", -otu) %>% 
  group_by(otu) %>%
  summarise(max = max(prop)) %>% 
  ungroup() %>% 
  filter(max < 0.0001) %>% # minimal proportional threshold 0.01% 
  pull(otu)
  
# remove OTU not passing 0.01% threshold 
pcount_otu_fl <- pcount_otu %>% 
  rownames_to_column("otu") %>% 
  filter(!otu %in% otu_id_dropped) %>% 
  column_to_rownames("otu")

```

### Step 5. Perform the clr transformation

```{r}

# extract the descendingly ordered taxa 
desc_otu_id <- pcount_otu_fl %>% 
  rownames_to_column("otu") %>% 
  gather(key = "sample", value = "pcount", -otu) %>% 
  group_by(otu) %>% 
  summarise(total = sum(pcount)) %>% 
  arrange(desc(total)) %>% 
  pull(otu)


# re-order the pcount OTU table 
pcount_otu_fl_desc <- pcount_otu_fl[desc_otu_id, ]


# clr transformation 
pcount_otu_fl_desc_clr <- t(apply(pcount_otu_fl_desc, 2, function(x) {log(x) - mean(log(x))}))  

pcount_otu_fl_desc_clr <- data.frame(pcount_otu_fl_desc_clr)  # convert to data frame format 


```

### Step 6. Princinpal component analysis of clr-transformed OTU matrix

Conduct principal component analysis on the compositional data set using
`prcomp()`

```{r pca, results='hide'}

pca_clr <- prcomp(pcount_otu_fl_desc_clr) 

summary(pca_clr) 

```

### Step 7. PCA plot

```{r PCA plot}

# extract PC1-2 for all samples 
pc2 <- data.frame(pca_clr$x[,1:2]) 

# merge metadata 
pc2_meta <- pc2 %>% 
  rownames_to_column("sample") %>% 
  inner_join(meta, by = c("sample" = "X.SampleID")) %>% 
  rename(is_obese = Var) %>% 
  mutate(is_obese = factor(is_obese, levels = c("Obese", "Lean"))) %>% 
  select(-ControlVar)

```

Produce PCA plot with `ggplot2` package.

```{r ggplot PCA, fig.height=6, fig.width=8}

# plotting with ggplot2 
ggplot(pc2_meta, aes(x=PC1, 
                     y=PC2, 
                     color=is_obese))+
  geom_point(alpha = 0.7, 
             size=3) +
  stat_ellipse()+ # add ellipses 
  scale_color_manual(values = c("#0000FF", "#ED003E"),
                     name = "Experimental\nCondition",  # change text of legend title 
                     labels = c("obese", "lean")) + # change text of legend labels 
  scale_y_continuous(limits = c(-25, 25)) +  # change the y-axis ranges 
  scale_x_continuous(limits = c(-25, 25)) +  # change the x-axis ranges 
  labs(title = "Principal Component Analysis of clr-transformed OTU matrix",
       subtitle = "Effect of obesity on gut microbiome compositions",
       caption = "The PCA was conducated upon clr-transformed OTU matrix.") + 
  theme(panel.background = element_blank(),
        panel.border = element_rect(colour = "black", fill = NA, size = 1),
        plot.caption = element_text(hjust = 0))
  

```

### Step 8. Scree plot of princinpal component analysis

Use [scree
plot](https://en.wikipedia.org/wiki/Scree_plot#:~:text=In%20multivariate%20statistics%2C%20a%20scree,principal%20component%20analysis%20(PCA).)
to display the proportion of the total variation in the data set that is
explained by each of the components in a principal component analysis.

Create the scree plot using `ggplot2` following the online
[instruction](https://www.statology.org/scree-plot-r/).

```{r scree plot}

# calculate total variance explained by each principal component 
var_explained <- pca_clr$sdev^2 / sum(pca_clr$sdev^2)  

var_explained[1:4]  # take a look at variations of four PCs 

```

The first principal component explains 9.0% of the total variation in
the data set; the second principal component explains 6.4% of the total
variation in the data set.

```{r ggplot2 scree plot, collapse=TRUE}

# store var_explained in the data frame 
var_explained_df <- data.frame(PC = c(1:10),
                               var_exp = var_explained[1:10])  


ggplot(var_explained_df, 
       aes(x= PC, 
           y= var_exp)) +
  geom_point() +
  geom_line() +
  xlab("Principal Component") +
  ylab("Variance Explained") +
  scale_x_discrete(limits = c(1:10)) + 
  scale_y_continuous(limits = c(0.01, 0.1),
                     labels = scales::percent)+ 
  geom_vline(xintercept = 4, color = "red", linetype = "dashed")+
  ggtitle("Scree Plot") +
  theme(panel.background = element_blank(),
        panel.border = element_rect(colour = "black", fill = NA, size = 1))


```

According to the scree test, the "elbow" of the graph where the
eigenvalues seem to level off is found and factors or components to the
left of this point should be retained as significant.

### Step 9. Hierarchical cluster analysis

Hierarchical cluster analysis (e.g., principles, methods and
implementations) is well documented in
[GustaMe](https://sites.google.com/site/mb3gustame/dissimilarity-based-methods/cluster-analysis/hierarchical-cluster-analysis),
and widely used in exploratory data analysis of microbiome data.

Conduct a cluster analysis and plot cluster dendrogram on dissimilarity
based on the `clr-transformed` OTU matrix. Use `Euclidian` distance
because the Aitchison transformed data are linearly related, but all
distances should be calculated from the ratios.

```{r cluster dist}

# calculate distance matrix 
dist <- dist(pcount_otu_fl_desc_clr,  # input clr-transformed OTU data 
             method = "euclidian")  # Euclidian distance 

# cluster the data 
hc_ward.d2 <- hclust(dist, # dissimilarity structure as produced by dist 
                     method = "ward.D2") # the agglomeration algorithm used 

```

Draw dendrogram with the base R function and ggplot2-based function as
alternative, following the instructions on
[STHDA](http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning)
and
[stackoverflow](https://stackoverflow.com/questions/49088627/how-to-label-colored-bars-in-a-dendrogram).

```{r dendrogram base R, collapse=TRUE, fig.dim=c(8,6)}

# create dendrogram object 
dend_ward.d2 <- hc_ward.d2 %>% 
  as.dendrogram  

# # get the labels of the tree/ dendrogram 
# labels(dend_ward.d2) 

# extract the grouping variable for labels 

label_id <- data.frame(X.SampleID = labels(dend_ward.d2))  # extract tree labels 

label_meta <- plyr::join(label_id, meta)  # full join two data sets keeping original order of tree labels 

# assign node color by experimental conditions
node_col_df <- label_meta %>% 
  mutate(node_col = case_when(
    Var == "Obese" ~ "blue",
    Var == "Lean" ~ "red",
    is.na(Var) ~ "grey"
  )) %>% 
  mutate(bar_code = case_when(
    Var == "Obese" ~ 1,
    Var == "Lean" ~ 2,
    is.na(Var) ~ 3
  ))


# Set the plot margin: bottom, left, top & right
par(mar = c(10, 3, 3, 4) + 0.1,
    xpd = NA) # allow content to go into outer margin

# dendrogram plot without labels 
dend_ward.d2 %>% 
  set("labels", '') %>%
  set("branches_k_color", k=3) %>% 
  plot 

# set up the color bar 
node_col2 <- node_col_df %>% pull(node_col) 
colored_bars(colors = node_col2, dend = dend_ward.d2, y_shift = -5)   

# add legend manually
legend("topright", legend = c('obese', 'lean', 'NA'), 
       pch = 15, 
       pt.cex = 1, 
       cex = 1, 
       bty = 'n',
       inset = c(-0.1, 0), # place outside
       title = "Experimental Conditions", 
       col = c('blue', 'red', 'grey'))


```

### Step 10. Bar plot of microbiota composition

Use Aitchison composition, `acomp()` function to analyze compositions in
the philosophical framework of the Aitchison Simplex. The input to
`acomp()` is absolute count (read) data in the sample X taxa format. The
output of `acomp()` represents one closed composition or a matrix of
class `acomp` represents multiple closed compositions each in one row.

The website,
[rdocumentation](https://www.rdocumentation.org/packages/compositions/versions/2.0-4/topics/acomp)
provides a code example of running `acomp()` function.

```{r bar plot, fig.dim=c(8,6)}
# reorder according to clustering structure 
pcount_otu_fl_reorder <- pcount_otu_fl[ ,hc_ward.d2$order]  # the input data is OTU count data after 0 replacement


# create acomp vector  
re_order_acomp <- acomp(t(pcount_otu_fl_reorder))  # row sum of acomp output is 1 



# display top 11 taxa based on mean relative abundance; while combining all other taxa as a single group 'others'  
re_order_acomp_df <- data.frame(re_order_acomp) 

# top11 taxa 
top11_taxa <- re_order_acomp_df %>% 
  rownames_to_column("sample") %>% 
  gather(key = "otu", value = "rel_abund", -sample) %>% 
  group_by(otu) %>% 
  summarise(total = sum(rel_abund)) %>% 
  arrange(desc(total)) %>% 
  head(11) %>%
  pull(otu)

rel.abund_regroup <- re_order_acomp_df %>% 
  rownames_to_column("sample") %>% 
  gather(key = "otu", value = "rel_abund", -sample) %>% 
  mutate(otu2 = case_when(
    otu %in% top11_taxa ~ otu,
    !otu %in% top11_taxa ~ "others"
  )) %>% 
  group_by(sample, otu2) %>% 
  summarise(rel_abund2 = sum(rel_abund)) 


# create the bar plot using ggplot2 

top11_taxa_shorten <- c("Faecalibacterium_prausnitzii_strain_ATCC_27768",
                        "Bacteroides_vulgatus_strain_JCM_5826",
                        "Eubacterium_rectale_strain_ATCC_33656",
                        "Blautia_luti_strain_DSM_14534",
                        "Ruminococcus_faecis_strain_Eg2",
                        "Subdoligranulum_variabile_strain_BI_114",
                        "Prevotella_copri_strain_CB7",
                        "Bacteroides_uniformis_strain_JCM_5828",
                        "Blautia_wexlerae_strain_DSM_19850",
                        "Eubacterium._hallii_strain_ATCC_27751",
                        "Bacteroides_dorei_strain_175",
                        "Others")

ggplot(rel.abund_regroup,
       aes(x= sample, 
           y= rel_abund2, 
           fill=otu2))+
  geom_bar(position = "stack", stat = "identity", color = NA) +
  # remove x-axis labels 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  # remove axis titles
  theme(axis.title = element_blank()) +
  # y-axis percentage 
  scale_y_continuous(labels = percent) + 
  # legend with shorten taxa names, below figure 
  scale_fill_brewer(palette = "Set3",
                    labels = top11_taxa_shorten) + 
  # color palette, Set3 
  # remove samples with missing data 
  theme(plot.margin = margin(1,1,0.5,0.5, "cm")) +
  # deal with legends 
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(nrow = 4, byrow = T)) +
  theme(legend.key.size = unit(0.3, "cm"),
        legend.text = element_text(size = 7),
        legend.title = element_blank()) 
  


  
```

### Step 11. Perform the Welch's t and Wilcoxon Rank Sum Test using `aldex.ttest()`

ALDEx2 needs the input data with taxa by samples formats.

```{r input aldex2, results='hide'}

# metadata 
is.obese <- meta %>% 
  rename(sample = X.SampleID, 
         is_obese = Var) %>% 
  select(-ControlVar)

# otu count data 
na_t_raw_otu2 <- na_t_raw_otu %>% 
  rownames_to_column("sample") 

# merge metadata and count data 
groups <- na_t_raw_otu2 %>% 
  inner_join(is.obese, by= "sample") %>% 
  pull(is_obese) 

length(groups)  # 142 samples 

groups <- factor(groups)


# subset count data 
na_t_raw_otu3 <- na_t_raw_otu2 %>% 
  inner_join(is.obese, by= "sample") %>% 
  select(-is_obese) %>% 
  gather(key = "otu", value= "reads", -sample) %>% 
  spread(key = "sample", value = "reads") %>% 
  column_to_rownames("otu") 

ncol(na_t_raw_otu3) # 142 samples 

# input count data to ALDEX2 
aldex_raw_count <- na_t_raw_otu3 


```

Generate instances of the centered log-ratio transformed values using
the function `aldex.clr()`

The function `aldex.clr()` has three input: count table, number of
Monte-Carlo instances, and level of verbosity (TRUE or FALSE). The
package authors recommend 128 or more `mc.samples` for the t-test, 1000
for a rigorous effect size calculation, and at least 16 for ANOVA.

```{r aldex clr}

vdr <- aldex.clr(aldex_raw_count,
                 groups,
                 mc.samples = 128,
                 verbose = TRUE)  

```

Perform the Welch's and Wilcoxon rank sum test using `aldex.ttest()`

The function `aldex.ttest()` returns the values of `we.ep` (expected
p-value of Welch's t test), `we.eBH` (expected Benjamini-Hochberg
corrected p-value of Welch's t test), `wi.ep` (expected p-value of
Wilcoxon rank sum test), and `wi.eBH` (expected Benjamini-Hochberg
corrected p-value of Wilcoxon rank sum test)

The function, `aldex.glm()` is alternative to `aldex.ttest()` which
implement Kruskal-Wallis test. However, it is slow.

```{r}

vdr_t <- aldex.ttest(vdr, paired.test=FALSE, hist.plot=FALSE, verbose=FALSE)   

head(vdr_t)
```

Estimate effect size using the function `aldex.effect()`

The `aldex.effect()` function estimates effect size and the within and
between condition values in the case of two conditions.

It has 3 arguments: - the aldex object from aldex.clr() - a flag
indicating whether or not to include values for all samples are used as
the denominator - the level of verbosity

```{r effect size}

vdr_effect <- aldex.effect(vdr, include.sample.summary = FALSE, verbose = FALSE)

```

The function, `aldex.effect()` returns all values including, - rab.all
median clr value for all samples inthe feature - rab.win.VdrFecal median
clr value for the VdrFecal group of samples - rba.win.VdrCecal median
clr value for the VdrCecal groups of samples - dif.btw median difference
in clr values beween VdrFecal and VdrCecal groups - dif.win median of
the largest difference in clr values within VdrFecal and VdrCecal
groups - effect median effect size: diff.btw/max(diff.win) for all
instances - overlap proportion of effect size that overlaps 0: it is
overlap between the Bayesian distribuiotn of rous VdrFecal and VdrCecal

Merge all outputs into one single object and make a data frame for
result viewing and downstream analysis

```{r}

vdr_all <- data.frame(vdr_t, vdr_effect)  


head(vdr_all)
```

Check any significant taxa between obese and lean conditions detected in
both Welch's t-test and Wilcoxon rank sum tests.

```{r sig taxa}

sig_by_both <- vdr_all %>% 
  filter(we.ep <= 0.05 & wi.ep <= 0.05)

sig_by_both 


```

Any taxa remain significant when p-values are adjusted for multiple
testing correction using BH method.

```{r pval sig}

sig_by_both_fdr <- vdr_all %>% 
  filter(we.eBH <= 0.05 & wi.eBH <= 0.05) 

sig_by_both_fdr 

```

### Step 12. Bland-Altman plot (also known as "difference plot")

The point underlying the method is that any two methods designing to
measure the same property or parameter should have agree sufficiently
closely, but not merely highly correlated.

ALDEX2 provides a Bland-Altman (MA) style plot to graphically compare
the degree of agreement of measures between median log2
between-condition difference and median log2 relative abundance.

```{r MA plot}

aldex.plot(vdr_all, 
           type = "MA", # specifies plot type to be produced 
           test = "welch", # indicates using Welch's t test to calculate significance 
           cutoff.pval = 0.15, 
           all.cex = 0.7, # symbol size
           called.cex = 1.1, # specify the character expansion of points with FDR, q<= 0.1
           rare.col = "grey", # grey for rare taxa 
           called.col = "red")  # present those taxa that have a mean BH adjusted Wilcoxon rank sum test p-value of 0.15 or less 

```

Effect size and effect size plot In ALDEX2, the effect size is defined
as a measure of the mean ratio of the difference between groups
(diff.btw) and the maximum difference within groups (diff.win or
variance).

```{r}

par(mfrow = c(1,2)) 
aldex.plot(vdr_all, 
           type = "MW",
           test = "welch",
           cutoff.pval = 0.15,
           all.cex = 0.7,
           called.cex = 1.1,
           rare.col = "black",
           called.col = "red") 
aldex.plot(vdr_all, 
           type = "MW",
           test = "wilcox",
           cutoff.pval = 0.15,
           all.cex = 0.7,
           called.cex = 1.1,
           rare.col = "black",
           called.col = "red")


```

In general, p-value is less robust than effect size. Thus, researchers
prefer to report effect size more often than the p-value. If sample size
is sufficiently large, an effect size of 0.5 or greater is considered
more likely corresponding to biological relevance.

In ALDEX2, an effect size cutoff of 1.5 - 2 and an overlap cutoff of
0.01 is considered as more appropriate to identify differential taxa of
interest.

Here, illustrate two additional plots about effect size: 1. plot the
effect size versus the p-value 2. a volcano plot to show difference
between groups versus p-values

```{r effect size plot}

par(mfrow = c(1,2)) 
plot(vdr_all$effect,
     vdr_all$wi.ep,
     log="y",
     pch=19,
     main = "Effect",
     cex=0.5,
     xlab = "Effect size",
     ylab= "Expected P value of Wilcoxon rank test") 
abline(h = 0.05, lty=2, lwd = 3, col="red") 
plot(vdr_all$diff.btw, vdr_all$wi.ep, 
     log="y",
     pch=19,
     main = "Volcano",
     cex=0.5,
     xlab="Difference",
     ylab="Expected P value of Wilcoxon rank test") 
abline(h=0.05, lty=2, lwd=3, col="red")


```

```{r package version info}
# Session information 
sessionInfo()  
```
